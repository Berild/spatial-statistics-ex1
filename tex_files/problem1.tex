We consider a continuous spatial variable in $\mathbb{R}^1$ s.t. 
\begin{align*}
    \{r(x): x \in \text{D} &: [1,50] \subset \mathbb{R}^1\} \\
    \E\{r(x)\} &= \mu_r = 0\\
    \Var\{r(x)\} & = \sigma_r^2 \\
    \Corr\{r(x), r(x')\} & = \rho_r(\tau) \, ,
\end{align*}

where $\tau = |x-x^\prime|/10.$ 
Discretization: $\text{L} \in \{1,2,...,50\}$.
The discretized random field (RF) is 
\begin{equation*}
    \{r(x): x \in \text{L}\}.
\end{equation*}

\paragraph{a)}
A function $c(\vect{\tau}) : \R^q \rightarrow \R$ is positive definite (non-negative definite, to be precise) for $\vect{\tau} \in \R^q$ if
%
\begin{equation*}
\begin{split}
    & \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j c(\vect{x_i} - \vect{x_j}) \geq 0 \\
    & \forall [\vect{x}_1, \vect{x}_2, \dots, \vect{x}_n] \in \R^{q \times n} \\
    & \forall \vect{\alpha} \in \R^n \\
    & \forall n \in \mathbb{N}_+ \setminus \{1\} \, .
\end{split}
\end{equation*}
%
In one dimension we replace the vectors $\vect{\tau}, \vect{x}_i$ and $\vect{x}_j$ with scalars $\tau, x_i$ and $x_j$.

Now assume that $\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \rho_r(\tau) < 0$ for some $[x_1, x_2, \dots, x_n]$, $\vect{\alpha}$ and $n$. Then
%
\begin{align*}
    \Var\left(\sum_{i=1}^n \alpha_i r(x_i)\right) 
    &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \Cov\{r(x_i), r(x_j)\} \\
    &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \sigma^2_r \Corr\{r(x_i), r(x_j)\} \\
    &= \sigma^2_r \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \rho_r(\tau) \\
    &< 0 \, .
\end{align*}
%
But the variance of a weighted sum of random variables is non-negative, so this is a contradiction. Thus the correlation function needs to be positive definite.

Two possible correlation functions, $\rho_r(\tau)$, the Powered exponential and the Matern. The functions take values for $\tau$, and a parameter $\nu_r$. Let $\nu_r \in \{1,3\}$ for the Matern function and $\nu_r \in \{1,1.9\}$ for Powered exponential. 

The correlation functions for each $\nu_r$ are shown in figure \ref{fig:corrfunc}. We see that the correlation is 1 for $\tau = 0$ and goes to 0 as $\tau$ increases for all the function and parameter configurations, as desired.

\begin{figure}
    \centering
    \includegraphics{figures/corrfunc.pdf}
    \caption{The Powered exponential and the Matern spatial correlation functions for some parameter values.}
    \label{fig:corrfunc}
\end{figure}

The variogram function is
%
\begin{align*}
\gamma_r(\tau)
\coloneqq&\: \frac{1}{2} \Var \{ r(x) - r(x') \} \\
=&\: \sigma_r^2 - \Cov\{r(x), r(x')\} \\
=&\: \sigma_r^2[1 - \rho_r(\tau)] \, ,
\end{align*}
%
i.e. one minus the covariance scaled by the variance. This is displayed for the given parameter values in figure \ref{fig:variograms}.

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/variograms.pdf}
    \caption{The variogram functions corresponding to Powered exponential and the Matern spatial correlation functions for some parameter values. The variance, $\\sigma^2$, is purely scaling the functions.}
    \label{fig:variograms}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{b)}
The discretized prior Gaussian model has pdf
\begin{equation}
    \vect{r} \sim p(\vect{r}) = \phi_n(\vect{r}; \mu_r \vect{i}_n, \sigma_r^2\matr{\Sigma}_r^{\rho}),
\end{equation}
where $n = 50$ is the number of grid points in $\text{L},$ $\vect{i}_n$ is a $n$-vector with $1$s and $\matr{\Sigma}_r^{\rho}$ is the correlation matrix for the grid points, defined from the chosen spatial correlation function applied to all $\tau$s in the system.

Ten realizations for each of the eight different parameter combinations are displayed in figures \ref{fig:prior_samp_exp} and \ref{fig:prior_samp_matern}. The priors in figure \ref{fig:prior_samp_exp} are using the Powered exponential function as correlation function, while the ones in figure \ref{fig:prior_samp_matern} use the Matern correlation function.

The influence of the parameter values are illustrated in the plots. Large $\sigma^2$ means large variability in values, and the realizations vary more along the $y$-axis in these displays. Increased $\nu_r$ increases the correlation between space points, as shown in figure \ref{fig:corrfunc}. Thus, this results in smoother realizations, where changes are done more gradually.

\begin{figure}
\centering
    \begin{subfigure}[H]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.49\textwidth}  
        \centering 
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf2.pdf}
    \end{subfigure}

    \vskip\baselineskip
    
     \begin{subfigure}[H]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf3.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.49\textwidth}  
        \centering 
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf4.pdf}
    \end{subfigure}

    \caption{Realizations from prior Gaussian RF using Powered exponential correlation function. Model parametere values are shown the each plot, respectively.}
    \label{fig:prior_samp_exp}
\end{figure}

\begin{figure}
\centering
    \begin{subfigure}[H]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf5.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.49\textwidth}  
        \centering 
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf6.pdf}
    \end{subfigure}

    \vskip\baselineskip
    
     \begin{subfigure}[H]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf7.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.49\textwidth}  
        \centering 
        \includegraphics[scale=0.5,trim=0cm 0cm 0cm 0cm]{figures/sample1conf8.pdf}
    \end{subfigure}

    \caption{Realizations from prior Gaussian RF using Matern correlation function. Model parametere values are shown the each plot, respectively.}
    \label{fig:prior_samp_matern}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{c)}
Now assume the spatial variable is observed as 
\begin{align*}
    \{&d(x): x \in \{10,25,30\} \subset \text{L} \}\\
    &d(x) = r(x) + \epsilon(x),
\end{align*}
where the measurement errors, $\epsilon(\cdot)$, are independent and identically distributed.
\begin{equation*}
    \epsilon(x) \sim \phi(\epsilon; 0,\sigma^2_{\epsilon}).
\end{equation*}
$r(x)$ and $\epsilon(x^\prime)$ are also independent for all $x,x^\prime$.

The likelihood function for this model is the Gauss-linear likelihood.
\begin{equation}
    [\vect{d}|\vect{r}] = \matr{H}\vect{r} + \vect{\epsilon} \sim p(\vect{d}|\vect{r}) = 
    \phi_m\left(\vect{d};\matr{H}\vect{r}, \matr{\Sigma}_{d|r}\right).
\end{equation}
$m$ is the dimension of $\vect{d}|\vect{r}$, $\matr{H}$ is the observation matrix with dimensions $m \times n$, and $\matr{\Sigma}_{d|r} = \sigma^2_{\epsilon} \matr{I}_m$, $\matr{I}_m$ being the $m \times m$ identity matrix.

The likelihood $p(\vect{d}|\vect{r})$ would be a pdf as a function of the data, but in this setting, the data is known and constant and $\vect{r}$ is unknown. Thus, the likelihood should be considered a function of $\vect{r}$. As a function of $\vect{r}$, it does noe necessarily integrate to $1$, and hence it is not a pdf. Because of this, there is no need to normalize the likelihood function.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{d)}
To simulate that $\{d(x): x \in \{10,25,30\} \subset \text{L} \}$  is observed, one realization from problem 1b) is saved. The realization is chosen from the prior with parameter combination $\sigma^2 = 5$ and Powered exponential correlation function with $\nu_r = 1.9$. We also let $\sigma^2_{\epsilon} \in \{0,0.25\}$.

The Gaussian RF is a conjugate prior class to the Gauss-linear likelihood. Thus, the posterior distribution is also a Gaussian RF. The Gaussian distribution is completely determined by the mean and the variance. For the discretized version we have
\begin{align*}
    [\vect{r}|\vect{d}] &\sim p(\vect{r}|\vect{d}) = \phi_n\left(\vect{r};\vect{\mu}_{r|d}, \matr{\Sigma}_{r|d}\right), \text{ with }\\
    \vect{\mu}_{r|d} &= \mu_r\vect{i}_n + \sigma^2_r\matr\Sigma^{\rho}_r\matr{H}^T \left[\sigma^2_r\matr{H}\matr\Sigma^{\rho}_r\matr{H}^T + \matr{\Sigma}_{d|r}\right]^{-1}[\vect{d}-\mu_r\matr{H}\vect{i}_n]\\
    \matr{\Sigma}_{r|d} &= \sigma^2_r\matr\Sigma^{\rho}_r - \sigma^2_r\matr\Sigma^{\rho}_r\matr{H}^T \left[\sigma^2_r\matr{H}\matr\Sigma^{\rho}_r\matr{H}^T + \matr{\Sigma}_{d|r}\right]^{-1} \sigma^2_r\matr{H} \matr\Sigma^{\rho}_r
\end{align*}

For prediction, the posterior mean is commonly used. The associated $(1-\alpha)$ prediction interval is 
$$\left[\vect{\mu_{r|d}}-z_{\alpha/2} \sqrt{diag(\matr{\Sigma_{r|d}})},\quad \vect{\mu_{r|d}}+z_{\alpha/2} \sqrt{diag(\matr{\Sigma_{r|d}})}\right].$$

Predicted values with corresponding prediction intervals for the two measurement errors are shown in figure \ref{fig:predictions}.

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/predictions.pdf}
    \caption{Posterior mean and prediction intervals for two different measurement errors when measured in $x = \{10,25,30\}$. The measured values are shown in black dots.}
    \label{fig:predictions}
\end{figure}

The measurement error has an effect in the posterior predictions in several ways. The measurement errors means that different values can be drawn from the same system in the same positions, and the error affects both the mean and the variance in the posterior distribution. In particular, notice that when the measurement error $\sigma^2_{\epsilon} = 0$, the posterior mean in the measured positions matches the measured values in this places, and there is no uncertainty in these positions. The uncertainty increases with the distance from the measured positions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{e)}

The predictions from problem 1d) can be estimated by simulate realizations from the posterior distribution and computing the empirical means, $\vect{\hat\mu}_{r|d}$, and standard deviations ($\vect{\hat\sigma}^2_{r|d}$) from the realizations. 

Consider the prediction of one grid point in $L$, $y_i$. Then $y_i-\hat\mu_{r|d,i}$ is Gaussian distributed with $\text{E}[y_i-\hat\mu_{r|d,i}] = 0$ and $\text{Var}(y_i-\hat\mu_{r|d,i}) = \sigma^2_{r|d,i}(1+1/n)$, with $n$ being the number of simulations. Thus

\begin{align*}
    &\frac{y_i - \hat\mu_{r|d,i}}{\sigma_{r|d,i}\sqrt{1+1/n}} \sim \phi(0,1)\\
     &\frac{y_i - \hat\mu_{r|d,i}}{\hat\sigma_{r|d,i}\sqrt{1+1/n}} \sim t_{n-1}\\
     \implies \hat{PI_{\alpha}} &= \left[\hat\mu_{r|d,i} - t_{n-1,\alpha/2}\cdot\hat\sigma_{r|d,i}\sqrt{1+1/n}, \hat\mu_{r|d,i} + t_{n-1,\alpha/2}\cdot \hat\sigma_{r|d,i}\sqrt{1+1/n}\right]
\end{align*}



\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/posteriorSamps.pdf}
    \caption{$100$ realizations from the posterior distribution for two different measurement errors when measured in $x = \{10,25,30\}$. Empirical estimated means and prediction intervals are printed on top of the realizations.}
    \label{fig:postSamps}
\end{figure}

The realizations, empirical prediction and empirical prediction interval are shown in figure \ref{fig:postSamps}. The figure shows how the measurement error affects the posterior distribution and the realizations thereof. When the measurement error is $0$, there is not uncertainty in the measured points, and all realizations will have the same value in $x = \{10,25,30\}$. There is more variability in the realizations of the model where some measurement was present, as expected. The empirical mean depends on the realizations drawn, but with with increased number of realizations, the empirical mean will converge to the theoretical mean. The empirical prediction intervals are larger than the theoretical one, because of the larger tail in the t-distribution and added uncertainty. This can be seen on the scales of figure \ref{fig:postSamps} versus figure \ref{fig:predictions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{f)}
Consider the following function on $\{r(x); x \in \text{D}\}$:
\begin{equation}
    A_r = \int_{\text{D}} I(r(x)>2)dx,
\end{equation}
with $I$ being the indicator function that is $1$ for $r(x)>2$ and $0$ otherwise. Let $A_{r,d} = \sum_{x \in L} I(r(x)>2)$ be the discrete version of $A_r$.

We will use the posterior model for $\{r(x); x \in \text{D}\}$ problem 1d)-e) where the $\sigma^2_{\epsilon} = 0$ and consider two predictors for $A_r$. One predictor is made from the $100$ simulated realizations, $\{r_i(x): x \in \text{L}, i \in [1,100]\}$, in problem 1e),
\begin{equation*}
    \hat{A}_r = \frac{1}{100}\sum_{i=1}^{100} \left(\sum_{x \in L} I(r_i(x)>2)\right) := \frac{1}{100}\sum_{i=1}^{100} A_{r_i,d},
\end{equation*}
and the other estimates $A_r$ using the predicted spatial variable $\hat{r}(x)$ from 1d):
\begin{equation*}
    \tilde{A}_r = \sum_{x \in L} I(\hat{r}(x)>2).
\end{equation*}

For the former predictor, we would also like to give the associated prediction variance. The variance of the predictor $\hat{A}_r$ is
\begin{equation*}
    \text{Var}(\hat{A}_{r}) = \frac{1}{100^2}\sum_{i=1}^{100}\text{Var}(A_{r_i,d}) = \frac{1}{100}\text{Var}(A_{r_i,d})
\end{equation*}

Now let $\hat{a}_r$ be the prediction of $\hat{A}_r$ given the 100 realizations from 1e), i.e a numerical value. Then the empirical variance for $A_{r,d}$ is 
\begin{equation*}
    \hat{\text{Var}}(A_{r,d}) = \frac{1}{100-1} \sum_{i=1}^{100}  (A_{r_i,d}-\hat{a}_r)^2, 
\end{equation*}
and the estimated variance of the predictor is 
\begin{equation*}
    \hat{\text{Var}}(\hat{A}_{r}) = \frac{1}{100}\hat{\text{Var}}(A_{r_i,d})
\end{equation*}

Inserting numerical values for the realizations gives the following predictions and the prediction variance:
\begin{align*}
    \hat{a}_r & = 22.07\\
    \tilde{a}_r & = 19\\
    \hat{\text{Var}}(\hat{A}_{r}) & = 0.3609\\
\end{align*}

We observe that the $\hat{a}_r > \tilde{a}_r$. 
Is it to be expected for the predictors in general that $\hat{A}_r > \tilde{A}_r$? 

Jensen's inequality states that for convex functions $g(X)$, the following inequality holds: 
\begin{equation*}
    \text{E}(g(X)) \geq g(E(X)).
\end{equation*}

\textbf{missing argument}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{g)}
text text text
